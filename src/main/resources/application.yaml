server:
  port: 8080
  name: DataIngestionService

ingestion:
  chunk-size: 200
  chunk-overlap: 40

embedding:
  batch-size: 20      # chunks per OpenAI API call (max 2048 tokens per input)
  parallelism: 4      # concurrent batch threads
  search:
    default-k: 5      # results returned when k is not specified
    max-k: 50         # upper bound to prevent runaway queries

rag:
  model: gpt-4o-mini   # LLM model to use for answer generation
  max-tokens: 1024      # max tokens in LLM response
  temperature: 0.2      # low temperature = more factual, less creative
  default-top-k: 5      # chunks to retrieve when topK not set in request

spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/roms_db
    username: roms_user
    password: roms_pass
    driver-class-name: org.postgresql.Driver

  jpa:
    database-platform: org.hibernate.dialect.PostgreSQLDialect
    hibernate:
      ddl-auto: validate
    show-sql: true
    properties:
      hibernate:
        format_sql: true

  flyway:
    enabled: true
    locations: classpath:db/migration

  servlet:
    multipart:
      max-file-size: 50MB
      max-request-size: 50MB
