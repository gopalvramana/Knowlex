# Knowlex

A document ingestion service built with Spring Boot that extracts text from documents, chunks it using a sliding window algorithm, and persists the results for downstream knowledge management use cases.

## Tech Stack

- **Java 17** / **Spring Boot 3.2.2**
- **Spring Data JPA** with H2 (default) or PostgreSQL
- **Apache PDFBox 2.0.30** for PDF text extraction
- **Lombok 1.18.42** for boilerplate reduction
- **Maven** build tool

## Architecture

```
PDF Upload â†’ Text Extraction â†’ Chunking â†’ Persistence
     â†“              â†“              â†“            â†“
  Controller    PdfBox        Sliding       JPA/H2
  (REST API)   Extractor      Window       Repository
```

The processing pipeline follows a layered architecture with interfaces at each stage:

| Layer | Interface | Implementation | Status |
|-------|-----------|----------------|--------|
| API | `DocumentIngestionController` | REST endpoints | âœ… Implemented |
| Service | `DocumentIngestionService` | `DocumentIngestionServiceImpl` | âœ… Implemented |
| Extraction | `DocumentTextExtractor` | `PdfBoxDocumentExtractor` | âœ… Implemented |
| Chunking | `TextChunker` | `SlidingWindowChunker` | âœ… Implemented |
| Persistence | `DocumentRepository` | Spring Data JPA | âœ… Implemented |
| Persistence | `DocumentChunkRepository` | Spring Data JPA | âœ… Implemented |
| Preprocessing | `TextPreprocessor` | `DefaultTextPreprocessor` | ðŸ”² Skeleton |
| Storage | `ChunkStorage` | `FileSystemChunkStorage` | ðŸ”² Skeleton |

## API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/documents` | Upload a PDF for ingestion (multipart/form-data) |
| `GET` | `/api/v1/documents` | List all ingested documents |
| `GET` | `/api/v1/documents/{id}` | Get document metadata by ID |
| `GET` | `/api/v1/documents/{id}/chunks` | Get all text chunks for a document |
| `DELETE` | `/api/v1/documents/{id}` | Delete a document and its chunks |

### Example Usage

```bash
# Upload a PDF
curl -X POST http://localhost:8080/api/v1/documents -F "file=@document.pdf"

# List all documents
curl http://localhost:8080/api/v1/documents

# Get chunks for a document
curl http://localhost:8080/api/v1/documents/{document-id}/chunks

# Delete a document
curl -X DELETE http://localhost:8080/api/v1/documents/{document-id}
```

### Response Examples

**POST** `/api/v1/documents` (201 Created):
```json
{
  "documentId": "c768c61c-0a95-459b-9414-f5d0782b1bc6",
  "filename": "document.pdf",
  "totalChunks": 47,
  "status": "COMPLETED",
  "createdAt": "2026-02-10T03:27:31.533224Z"
}
```

**GET** `/api/v1/documents/{id}/chunks` (200 OK):
```json
[
  {
    "id": "e1f38814-c580-4edd-afd7-8fea41788004",
    "documentId": "c768c61c-0a95-459b-9414-f5d0782b1bc6",
    "chunkIndex": 0,
    "content": "First chunk of extracted text..."
  }
]
```

## Project Structure

```
src/main/java/com/symphony/docweave/
â”œâ”€â”€ KnowlexApplication.java          # Entry point
â”œâ”€â”€ api/                              # REST controllers
â”‚   â”œâ”€â”€ DocumentIngestionController.java
â”‚   â”œâ”€â”€ GlobalExceptionHandler.java
â”‚   â””â”€â”€ dto/                          # Request/Response DTOs
â”‚       â”œâ”€â”€ IngestionResponse.java
â”‚       â”œâ”€â”€ DocumentResponse.java
â”‚       â”œâ”€â”€ ChunkResponse.java
â”‚       â””â”€â”€ ErrorResponse.java
â”œâ”€â”€ service/                          # Business logic
â”‚   â”œâ”€â”€ DocumentIngestionService.java (interface)
â”‚   â””â”€â”€ impl/
â”‚       â””â”€â”€ DocumentIngestionServiceImpl.java
â”œâ”€â”€ domain/                           # Domain models & JPA entities
â”‚   â”œâ”€â”€ Document.java
â”‚   â”œâ”€â”€ DocumentEntity.java
â”‚   â”œâ”€â”€ DocumentChunkEntity.java
â”‚   â”œâ”€â”€ DocumentChunk.java
â”‚   â””â”€â”€ DocumentType.java
â”œâ”€â”€ repository/                       # Spring Data JPA repositories
â”‚   â”œâ”€â”€ DocumentRepository.java
â”‚   â””â”€â”€ DocumentChunkRepository.java
â”œâ”€â”€ extractor/                        # Document text extraction
â”‚   â”œâ”€â”€ DocumentTextExtractor.java (interface)
â”‚   â””â”€â”€ PdfBoxDocumentExtractor.java
â”œâ”€â”€ chunker/                          # Text chunking algorithms
â”‚   â”œâ”€â”€ TextChunker.java (interface)
â”‚   â””â”€â”€ SlidingWindowChunker.java
â”œâ”€â”€ preprocessor/                     # Text preprocessing (skeleton)
â”œâ”€â”€ storage/                          # Chunk persistence (skeleton)
â”œâ”€â”€ config/                           # Spring configuration
â”‚   â””â”€â”€ IngestionProperties.java
â””â”€â”€ exception/                        # Custom exceptions
    â””â”€â”€ DocumentProcessingException.java
```

## Getting Started

### Prerequisites

- Java 17+
- Maven 3.8+

### Build & Run

```bash
# Build the project
mvn clean install

# Run the application (uses H2 in-memory database by default)
mvn spring-boot:run

# Run with PostgreSQL
mvn spring-boot:run -Dspring-boot.run.profiles=postgres

# Run tests
mvn test

# Package as JAR
mvn clean package
java -jar target/Knowlex-1.0-SNAPSHOT.jar
```

The application starts on **http://localhost:8080**.

### Database

**Default (H2):** No setup required. In-memory database starts automatically.
- H2 Console: http://localhost:8080/h2-console
- JDBC URL: `jdbc:h2:mem:knowlexdb`
- Username: `sa` / Password: _(empty)_

**PostgreSQL profile:** Create the database, then run with the `postgres` profile.

```sql
CREATE DATABASE roms_db;
CREATE USER roms_user WITH PASSWORD 'roms_password';
GRANT ALL PRIVILEGES ON DATABASE roms_db TO roms_user;
```

## Configuration

Configuration is managed via `src/main/resources/application.yaml`:

| Property | Default | Description |
|----------|---------|-------------|
| `server.port` | `8080` | Server port |
| `ingestion.chunk-size` | `200` | Words per chunk |
| `ingestion.chunk-overlap` | `40` | Overlapping words between chunks |
| `spring.datasource.url` | `jdbc:h2:mem:knowlexdb` | Database URL |
| `spring.servlet.multipart.max-file-size` | `50MB` | Max upload size |

## Data Model

### Documents

| Column | Type | Description |
|--------|------|-------------|
| `id` | UUID | Primary key |
| `source` | String | Content type of the uploaded file |
| `original_filename` | String | Original file name |
| `checksum` | String | SHA-256 checksum for deduplication |
| `created_at` | Instant | Creation timestamp |

### Document Chunks

| Column | Type | Description |
|--------|------|-------------|
| `id` | UUID | Primary key |
| `document_id` | UUID | Foreign key â†’ documents |
| `chunk_index` | int | Positional index within document |
| `content` | Text | Chunk text content |

## Key Features

- **PDF text extraction** via Apache PDFBox
- **Sliding window chunking** with configurable size and overlap
- **SHA-256 deduplication** â€” rejects duplicate file uploads
- **Structured error handling** â€” maps exceptions to proper HTTP status codes (400, 404, 409, 413, 422)
- **Multi-profile database** â€” H2 for development, PostgreSQL for production

## Roadmap

- [x] Implement REST API endpoints for document ingestion
- [x] Complete service layer orchestration
- [x] Configure environment profiles (H2 default, PostgreSQL opt-in)
- [x] Add JPA repositories with query methods
- [ ] Add text preprocessing (normalization, cleaning)
- [ ] Support additional document types (DOCX, HTML)
- [ ] Add comprehensive unit and integration tests
- [ ] Externalize database credentials
- [ ] Add Docker support
- [ ] Set up CI/CD pipeline
- [ ] Add OpenAPI/Swagger documentation

## License

This project is proprietary.
